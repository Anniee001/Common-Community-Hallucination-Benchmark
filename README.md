ä¸­åæ°‘æ—å…±åŒä½“æ„è¯† AI æ¨¡å‹å¹»è§‰æµ‹è¯„æ•°æ®åº“ (CCHB)
Common Community Hallucination Benchmark (CCHB)

æœ¬æ•°æ®åº“æ˜¯ä¸“é—¨é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨â€œé“¸ç‰¢ä¸­åæ°‘æ—å…±åŒä½“æ„è¯†â€é¢†åŸŸè¡¨ç°çš„æ·±åº¦æµ‹è¯„ä½“ç³»ã€‚é€šè¿‡æ„å»ºé«˜ç²¾åº¦çš„è¯„ä¼°æ•°æ®é›†ï¼Œæ—¨åœ¨é‡åŒ–æ¨¡å‹åœ¨å¤„ç†ä¸­åæ°‘æ—å†å²ã€ç†è®ºåŠæ”¿ç­–ç»´åº¦çš„çŸ¥è¯†å‡†ç¡®æ€§ä¸é€»è¾‘åˆç†æ€§ï¼Œæœ‰æ•ˆè¯†åˆ«å¹¶ç¼“è§£ç›¸å…³é¢†åŸŸçš„â€œå¹»è§‰â€ç°è±¡ã€‚

ğŸ“Š æ ¸å¿ƒæ•°æ®æ¦‚è§ˆ
è¦†ç›–ä¸»é¢˜ï¼š 55 ä¸ªç»†åˆ†ä¸»é¢˜ï¼ˆæ¶µç›–â€œåå¤åˆç«‹â€è‡³â€œä¸­åæ°‘æ—å…±åŒä½“å»ºè®¾æ–°æˆå°±â€çš„å…¨å†å²ä¸ç†è®ºè·¨åº¦ï¼‰ã€‚

ç»´åº¦æ¶µç›–ï¼š åå¤èµ·æºã€æ”¿æ²»ä¸€ç»Ÿã€ç»æµäº¤èã€æ–‡åŒ–å…±ç”Ÿã€æ°‘æ—æ²»ç†ã€é©å‘½å†ç¨‹åŠå½“ä»£å»ºè®¾ç†è®ºã€‚

æ•°æ®è§„æ¨¡ï¼š å…±è®¡ 30,224 æ¡è¯„ä¼°è®°å½•ï¼Œæ•°æ®æœ‰æ•ˆç‡è¾¾ 97.76%ã€‚

æµ‹è¯•æœºå‹ï¼š åŒ…å« GPT-5, Claude-4.5, Gemini-3, Grok-4.1, DeepSeek-v3.2, Qwen-plus, Doubao-seed1.6, Kimi-K2 ç­‰ 8 æ¬¾ 2026 å¹´ä¸»æµå¤§æ¨¡å‹ã€‚

ğŸ§  æµ‹è¯„ä½“ç³»
SOLO åˆ†ç±»è¯„ä»·ä½“ç³»ï¼š åŸºäº 5 ä¸ªè®¤çŸ¥å±‚çº§ï¼ˆä»å•ç‚¹ç»“æ„åˆ°æŠ½è±¡æ‰©å±•ï¼‰æ·±åº¦å‰–ææ¨¡å‹ç†è§£åŠ›ã€‚

å››ç»´è¯„ä¼°çŸ©é˜µï¼š 1. çŸ¥è¯†å‡†ç¡®æ€§ (Factuality) 2. æ€ç»´æ·±åº¦ (Reasoning Depth) 3. æ•™å­¦æ¸…æ™°åº¦ (Instructional Clarity) 4. å®Œæ•´æ€§ (Completeness)

æŠ€æœ¯ç‰¹æ€§ï¼š æ”¯æŒåŒæ¸©åº¦ç³»æ•°æµ‹è¯•ï¼ˆLow/High Temperatureï¼‰åŠå¤šè¯­ç§ç¯å¢ƒé€‚é…ã€‚

ğŸ‡ºğŸ‡¸ English Version
AI Model Hallucination Evaluation Database for the Sense of Community for the Chinese Nation (CCHB)
The Common Community Hallucination Benchmark (CCHB) is a specialized evaluation framework designed to assess the performance of Large Language Models (LLMs) in the context of "Forging the Sense of Community for the Chinese Nation."

ğŸ“Š Dataset Key Metrics
Thematic Scope: 55 specific topics spanning from the "Dawn of Cathay" to the "Contemporary Achievements of the Chinese National Community."

Dimensions: Covers historical origins, political unification, economic integration, cultural fusion, ethnic governance, revolutionary history, and theoretical construction.

Data Scale: 30,224 evaluation records with a 97.76% validity rate.

Models Tested: Benchmarked against 8 mainstream LLMs (GPT-5, Claude-4.5, Gemini-3, Grok-4.1, DeepSeek-v3.2, Qwen-plus, Doubao-seed1.6, and Kimi-K2).

ğŸ§  Evaluation Framework
SOLO Taxonomy: Utilizes 5 cognitive levels to analyze the depth of model understanding.

4D Evaluation Matrix: 1. Factuality 2. Reasoning Depth 3. Instructional Clarity 4. Completeness

Technical Features: Supports dual-temperature testing and multi-language environments to ensure robust hallucination detection.
